{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import librosa\n",
    "import numpy as np\n",
    "from transformers import Wav2Vec2ForCTC, Wav2Vec2Tokenizer\n",
    "\n",
    "# Load pre-trained model and tokenizer\n",
    "model = Wav2Vec2ForCTC.from_pretrained(\"facebook/wav2vec2-large-960h-lv60-self\")\n",
    "tokenizer = Wav2Vec2Tokenizer.from_pretrained(\"facebook/wav2vec2-large-960h-lv60-self\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load WAV file and extract features\n",
    "audio_file = \"harvard.wav\"\n",
    "signal, sample_rate = librosa.load(audio_file, sr=16000)\n",
    "inputs = tokenizer(signal, return_tensors=\"pt\", padding=True)\n",
    "\n",
    "# Pass features through the model to get predicted transcriptions\n",
    "with torch.no_grad():\n",
    "    outputs = model(inputs.input_values, attention_mask=inputs.attention_mask)\n",
    "\n",
    "predicted_ids = np.argmax(outputs.logits, axis=-1)\n",
    "transcription = tokenizer.batch_decode(predicted_ids)[0]\n",
    "\n",
    "# Identify timestamps of 'm' phoneme with confidence above a certain threshold\n",
    "m_indices = [i for i, (ph, conf) in enumerate(zip(transcription, outputs.logits.softmax(-1).max(-1).values)) if ph == 'm' and conf > 0.9]\n",
    "m_timestamps = [(idx * 0.02, (idx+1) * 0.02) for idx in m_indices]  # assuming frame shift of 0.02 seconds\n",
    "\n",
    "print(m_timestamps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchaudio\n",
    "import librosa\n",
    "import numpy as np\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "\n",
    "# Load the fine-tuned Facebook Wav2Vec2 model\n",
    "model = torch.hub.load('pytorch/fairseq', 'wav2vec2_large', 'facebook/wav2vec2-large-960h-lv60-self')\n",
    "\n",
    "# Load the test audio file\n",
    "test_file = \"harvard.wav\"\n",
    "waveform, sample_rate = torchaudio.load(test_file)\n",
    "\n",
    "# Preprocess the audio file\n",
    "resampler = torchaudio.transforms.Resample(sample_rate, 16000)\n",
    "waveform = resampler(waveform)\n",
    "feature_extractor = model.feature_extractor\n",
    "with torch.no_grad():\n",
    "    features = feature_extractor(waveform)\n",
    "features = np.array(features.squeeze(0))\n",
    "\n",
    "# Get the phonemes and their timestamps\n",
    "phonemes = []\n",
    "start_times = []\n",
    "end_times = []\n",
    "\n",
    "clustering = AgglomerativeClustering(n_clusters=None, distance_threshold=15, linkage='complete')\n",
    "cluster_labels = clustering.fit_predict(features)\n",
    "for i, label in enumerate(cluster_labels):\n",
    "    if i == 0 or label != cluster_labels[i-1]:\n",
    "        phoneme = model.task.target_dictionary.string([label + 4]) # +4 because the first 4 symbols are reserved for special tokens\n",
    "        if phoneme in ['m', 'b', 'p']:\n",
    "            phonemes.append(phoneme)\n",
    "            start_times.append((i * 10) / 1000) # start time in seconds\n",
    "    if i == len(cluster_labels) - 1 or label != cluster_labels[i+1]:\n",
    "        end_times.append(((i+1) * 10) / 1000) # end time in seconds\n",
    "\n",
    "# Combine the phonemes and their timestamps\n",
    "phoneme_times = [f\"{phoneme}-{start_time:.3f}\" for phoneme, start_time in zip(phonemes, start_times)]\n",
    "\n",
    "# Print the phonemes and their timestamps\n",
    "print(phoneme_times)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa\n",
    "import subprocess\n",
    "\n",
    "# Load WAV file and extract features\n",
    "audio_file = \"example.wav\"\n",
    "signal, sample_rate = librosa.load(audio_file, sr=16000)\n",
    "mfccs = librosa.feature.mfcc(y=signal, sr=sample_rate, n_mfcc=13, hop_length=256)\n",
    "\n",
    "# Reshape and write the features to disk as Kaldi input\n",
    "features = mfccs.transpose()\n",
    "with open(\"example.ark\", \"wb\") as f:\n",
    "    f.write(f\"{audio_file} [\".encode())\n",
    "    for i in range(features.shape[0]):\n",
    "        f.write(f\" {i} \".encode())\n",
    "        for j in range(features.shape[1]):\n",
    "            f.write(struct.pack(\"<f\", features[i,j]))\n",
    "    f.write(\"]\\n\".encode())\n",
    "\n",
    "# Use Kaldi to perform speech recognition and identify 'm' phoneme\n",
    "cmd = [\"online2-wav-nnet3-latgen-faster\", \"--rt-max=5\", \"--frames-per-chunk=20\", \"--acoustic-scale=1.0\",\n",
    "       \"--beam=12.0\", \"--lattice-beam=6.0\", \"--max-active=7000\", \"--min-active=200\", \"--do-endpointing=false\",\n",
    "       \"--config=exp/tdnn_lstm_asr_baseline_sp/config.yaml\", \"exp/tdnn_lstm_asr_baseline_sp/final.mdl\",\n",
    "       \"exp/tdnn_lstm_asr_baseline_sp/graph/HCLG.fst\", \"exp/tdnn_lstm_asr_baseline_sp/graph/words.txt\",\n",
    "       \"example.ark\", \"example.lat\"]\n",
    "subprocess.run(cmd)\n",
    "\n",
    "# Read the output lattice and identify 'm' phoneme\n",
    "with open(\"example.lat\", \"r\") as f:\n",
    "    for line in f:\n",
    "        if \"m\" in line:\n",
    "            fields = line.strip().split()\n",
    "            start_time = int(fields[2]) * 0.032\n",
    "            end_time = int(fields[3]) * 0.032\n",
    "            print(f\"m phoneme found at {start_time}-{end_time} seconds\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3rc1"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b081a66ee97bd2b6a16f43955f1d810b7ea816d6eaeb65e157ef9e038445f0c6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
