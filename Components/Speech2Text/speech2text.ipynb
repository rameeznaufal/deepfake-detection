{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Python code to convert video to audio\n",
    "import moviepy.editor as mp\n",
    "import torch\n",
    "import librosa\n",
    "import numpy as np\n",
    "import soundfile as sf\n",
    "import wave\n",
    "from scipy.io import wavfile\n",
    "from IPython.display import Audio\n",
    "from transformers import Wav2Vec2ForCTC, Wav2Vec2Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Writing audio in ./transcript.wav\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                      "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    }
   ],
   "source": [
    "# Insert Local Video File Path \n",
    "clip = mp.VideoFileClip(r\"./obamaDeepfake.mp4\")\n",
    "  \n",
    "# Insert Local Audio File Path\n",
    "clip.audio.write_audiofile(r\"./transcript.wav\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'Wav2Vec2CTCTokenizer'. \n",
      "The class this function is called from is 'Wav2Vec2Tokenizer'.\n",
      "Some weights of Wav2Vec2ForCTC were not initialized from the model checkpoint at facebook/wav2vec2-base-960h and are newly initialized: ['wav2vec2.masked_spec_embed']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = Wav2Vec2Tokenizer.from_pretrained(\"facebook/wav2vec2-base-960h\")\n",
    "model = Wav2Vec2ForCTC.from_pretrained(\"facebook/wav2vec2-base-960h\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wave\n",
    "import json\n",
    "\n",
    "from vosk import Model, KaldiRecognizer, SetLogLevel\n",
    "import Word as custom_Word\n",
    "\n",
    "model_path = \"models/vosk-model-en-us-0.21\"\n",
    "audio_filename = \"audio/speech_recognition_systems.wav\"\n",
    "\n",
    "model = Model(model_path)\n",
    "wf = wave.open(audio_filename, \"rb\")\n",
    "rec = KaldiRecognizer(model, wf.getframerate())\n",
    "rec.SetWords(True)\n",
    "\n",
    "# get the list of JSON dictionaries\n",
    "results = []\n",
    "# recognize speech using vosk model\n",
    "while True:\n",
    "    data = wf.readframes(4000)\n",
    "    if len(data) == 0:\n",
    "        break\n",
    "    if rec.AcceptWaveform(data):\n",
    "        part_result = json.loads(rec.Result())\n",
    "        results.append(part_result)\n",
    "part_result = json.loads(rec.FinalResult())\n",
    "results.append(part_result)\n",
    "\n",
    "# convert list of JSON dictionaries to list of 'Word' objects\n",
    "list_of_Words = []\n",
    "for sentence in results:\n",
    "    if len(sentence) == 1:\n",
    "        # sometimes there are bugs in recognition \n",
    "        # and it returns an empty dictionary\n",
    "        # {'text': ''}\n",
    "        continue\n",
    "    for obj in sentence['result']:\n",
    "        w = custom_Word.Word(obj)  # create custom Word object\n",
    "        list_of_Words.append(w)  # and add it to list\n",
    "\n",
    "wf.close()  # close audiofile\n",
    "\n",
    "# output to the screen\n",
    "for word in list_of_words:\n",
    "    print(word.to_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "result2:\n",
      "{   'alternative': [   {   'confidence': 0.8600024,\n",
      "                           'transcript': 'entering an error which are Enemies '\n",
      "                                         'committed look like anyone is saying '\n",
      "                                         'anything have me say things like '\n",
      "                                         'now'},\n",
      "                       {   'transcript': 'which are Enemies committed look '\n",
      "                                         'like anyone is saying anything have '\n",
      "                                         'me say things like now'},\n",
      "                       {   'transcript': 'entering an error which are Enemies '\n",
      "                                         'committed look like anyone saying '\n",
      "                                         'anything have me say things like '\n",
      "                                         'now'},\n",
      "                       {   'transcript': 'entering an error which are Enemies '\n",
      "                                         'committed look like anyone is saying '\n",
      "                                         'anything have me say things like now '\n",
      "                                         'in Assam'}],\n",
      "    'final': True}\n",
      "result2:\n",
      "{   'alternative': [   {   'confidence': 0.84630013,\n",
      "                           'transcript': 'someone we need to be more which we '\n",
      "                                         'trust from Yamuna trusted new '\n",
      "                                         'sources'},\n",
      "                       {   'transcript': 'spanks someone else someone we need '\n",
      "                                         'to be more which we trust from '\n",
      "                                         'Yamuna trusted new sources'},\n",
      "                       {   'transcript': 'someone else someone we need to be '\n",
      "                                         'more which we trust from Yamuna '\n",
      "                                         'trusted new sources'},\n",
      "                       {   'transcript': 'someone we need to be more which we '\n",
      "                                         'trust from Yamuna'},\n",
      "                       {   'transcript': 'spanks someone else someone we need '\n",
      "                                         'to be more which we trust from '\n",
      "                                         'Yamuna'}],\n",
      "    'final': True}\n",
      "result2:\n",
      "{   'alternative': [   {   'confidence': 0.87995279,\n",
      "                           'transcript': 'Ganpati defence between whatever we '\n",
      "                                         'become some kind of fuck up to stop '\n",
      "                                         'here thank you'},\n",
      "                       {   'transcript': 'Ganpati defence between whatever we '\n",
      "                                         'become some kind of fuck up to stop '\n",
      "                                         'here thank you stay work pictures'},\n",
      "                       {   'transcript': 'Ganpati defence between whatever '\n",
      "                                         'become some kind of fuck up to stop '\n",
      "                                         'here thank you'},\n",
      "                       {   'transcript': 'Ganpati defence between whatever we '\n",
      "                                         'become some kind of fuck up to stop '\n",
      "                                         'here thank you stay work better'},\n",
      "                       {   'transcript': 'Ganpati defence between whatever '\n",
      "                                         'become some kind of fuck up to stop '\n",
      "                                         'here thank you stay work better'}],\n",
      "    'final': True}\n",
      "result2:\n",
      "[]\n"
     ]
    },
    {
     "ename": "UnknownValueError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mUnknownValueError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-3532a0ee1491>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      8\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m         \u001b[1;32mwhile\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m             \u001b[0mtext\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrecognize_google\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maudio\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     11\u001b[0m             \u001b[0maudio\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrecord\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msource\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mduration\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m30\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0msr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mWaitTimeoutError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Python\\Python38\\lib\\site-packages\\speech_recognition\\__init__.py\u001b[0m in \u001b[0;36mrecognize_google\u001b[1;34m(self, audio_data, key, language, pfilter, show_all, with_confidence)\u001b[0m\n\u001b[0;32m    917\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'result2:'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    918\u001b[0m         \u001b[0mpprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mactual_result\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindent\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m4\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 919\u001b[1;33m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mactual_result\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mactual_result\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"alternative\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;32mraise\u001b[0m \u001b[0mUnknownValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    920\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    921\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;34m\"confidence\"\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mactual_result\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"alternative\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mUnknownValueError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import speech_recognition as sr\n",
    "\n",
    "r = sr.Recognizer()\n",
    "\n",
    "with sr.AudioFile('transcript.wav') as source:\n",
    "    audio = r.record(source, duration=30)\n",
    "    text = \"\"\n",
    "    try:\n",
    "        while True:\n",
    "            text += r.recognize_google(audio)\n",
    "            audio = r.record(source, duration=30)\n",
    "    except sr.WaitTimeoutError:\n",
    "        pass\n",
    "\n",
    "with open('output.txt', 'w') as f:\n",
    "    f.write(text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WE'RE ENTERING AN ERA IN WHICH OUR ENEMIES CAN MAKE IT LOOK LIKE ANY ONE IS SAYING ANYTHING AT ANY POINT IN TIME EVEN IF THEY WOULD NEVER SAY THOSE THINGS SO FOR INSTANCE THEY COULD HAVE ME SAY THINGS LIKE I'LL KNOW KILMONGER WAS RIGHT OR BEN CARSON IS IN THE SUNKIN PLACES OR ABOUT THIS SIMPLY PRESIDENT TRUMP IS A TOTAL AND COMPLETE DIPSHIT NOW YOU SEE I WOULD NEVER SAY THESE THINGS AT LEAST NOT IN A PUBLIC ADDRESS BUT SOME ONE ELSE WOULD SOME ONE LIKE GEORGAN PEAL THIS IS A DANGEROUS TIME MOVING FORWARD WE NEED TO BE MORE VIGILANT WITH WHAT WE TRUST FROM THE INNER NET THAT'S A TIME WHEN WE NEED TO RELIVE ON TRUSTED NEW SOURCES MAY SOUND BASIC BUT HOW WE MOVE FORWARD IN THE AGE OF INFORMATION IS GOING TO BE THE DIFFERENCE BETWEEN WHETHER WE SURVIVE OR WHETHER WE BECOME SOME KIND OF FUCKED UP DISTOPIA THANK YOU STAY WOKE BITCHES\n"
     ]
    }
   ],
   "source": [
    "file_name = 'transcript.wav'\n",
    "\n",
    "data = wavfile.read(file_name)\n",
    "framerate = data[0]\n",
    "sounddata = data[1]\n",
    "time = np.arange(0,len(sounddata))/framerate\n",
    "input_audio, _ = librosa.load(file_name, sr=16000)\n",
    "input_values = tokenizer(input_audio, return_tensors=\"pt\").input_values\n",
    "logits = model(input_values).logits\n",
    "predicted_ids = torch.argmax(logits, dim=-1)\n",
    "transcription = tokenizer.batch_decode(predicted_ids)[0]\n",
    "print(transcription)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "result2:\n",
      "{   'alternative': [   {   'confidence': 0.85035503,\n",
      "                           'transcript': 'entering an error which are Enemies '\n",
      "                                         'committed look like anyone is saying '\n",
      "                                         'anything'},\n",
      "                       {   'transcript': 'which are Enemies committed look '\n",
      "                                         'like anyone is saying anything'},\n",
      "                       {   'transcript': 'entering an error which are Enemies '\n",
      "                                         'committed look like anyone is saying '\n",
      "                                         'anything have me say things like '\n",
      "                                         'now'}],\n",
      "    'final': True}\n",
      "entering an error which are Enemies committed look like anyone is saying anything\n",
      "144.94\n",
      "Phoneme 'b', 'm', or 'p' found in word 'Enemies' at 0:55.746\n",
      "Phoneme 'b', 'm', or 'p' found in word 'committed' at 1:6.895\n"
     ]
    }
   ],
   "source": [
    "import speech_recognition as sr\n",
    "\n",
    "# Load the audio file\n",
    "r = sr.Recognizer()\n",
    "audio_file = sr.AudioFile('transcript.wav')\n",
    "with audio_file as source:\n",
    "    audio_data = r.record(source)\n",
    "\n",
    "# Perform speech recognition\n",
    "transcription = r.recognize_google(audio_data)\n",
    "\n",
    "print(transcription)\n",
    "\n",
    "# Split the transcription into words\n",
    "words = transcription.split()\n",
    "\n",
    "# Get the sample rate and number of samples from the audio data\n",
    "sample_rate = audio_data.sample_rate\n",
    "num_samples = len(audio_data.get_raw_data())\n",
    "\n",
    "# Calculate the duration of the audio data\n",
    "duration = num_samples / sample_rate\n",
    "\n",
    "print(duration)\n",
    "\n",
    "# Find the time stamps of the phonemes 'b', 'm', and 'p'\n",
    "for i, word in enumerate(words):\n",
    "    if 'b' in word or 'm' in word or 'p' in word:\n",
    "        word_duration = duration / len(words)\n",
    "        time_stamp = i * word_duration\n",
    "        minutes, seconds = divmod(time_stamp, 60)\n",
    "        milliseconds = int((seconds % 1) * 1000)\n",
    "        print(f\"Phoneme 'b', 'm', or 'p' found in word '{word}' at {int(minutes)}:{int(seconds)}.{milliseconds}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'acoustic_model.pt'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-24-080770189fd6>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     14\u001b[0m     \u001b[0mmfccs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlibrosa\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfeature\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmfcc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0maudio_data\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msr\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mrate\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_mfcc\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m13\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 16\u001b[1;33m     \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"acoustic_model.pt\"\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# load a pre-trained acoustic model\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     17\u001b[0m     \u001b[0minputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmfccs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mT\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# add a batch dimension to the inputs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m     \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# run the inputs through the acoustic model\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Python\\Python38\\lib\\site-packages\\torch\\serialization.py\u001b[0m in \u001b[0;36mload\u001b[1;34m(f, map_location, pickle_module, weights_only, **pickle_load_args)\u001b[0m\n\u001b[0;32m    769\u001b[0m         \u001b[0mpickle_load_args\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'encoding'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'utf-8'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    770\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 771\u001b[1;33m     \u001b[1;32mwith\u001b[0m \u001b[0m_open_file_like\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'rb'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mopened_file\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    772\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0m_is_zipfile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mopened_file\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    773\u001b[0m             \u001b[1;31m# The zipfile reader is going to advance the current file position.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Python\\Python38\\lib\\site-packages\\torch\\serialization.py\u001b[0m in \u001b[0;36m_open_file_like\u001b[1;34m(name_or_buffer, mode)\u001b[0m\n\u001b[0;32m    268\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0m_open_file_like\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    269\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0m_is_path\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 270\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0m_open_file\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    271\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    272\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;34m'w'\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Python\\Python38\\lib\\site-packages\\torch\\serialization.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, name, mode)\u001b[0m\n\u001b[0;32m    249\u001b[0m \u001b[1;32mclass\u001b[0m \u001b[0m_open_file\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_opener\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    250\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 251\u001b[1;33m         \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_open_file\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    252\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    253\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__exit__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'acoustic_model.pt'"
     ]
    }
   ],
   "source": [
    "import librosa\n",
    "import numpy as np\n",
    "import torch\n",
    "import wave\n",
    "import contextlib\n",
    "\n",
    "filename = 'transcript.wav'\n",
    "\n",
    "with contextlib.closing(wave.open(filename,'r')) as f:\n",
    "    rate, audio_data = wavfile.read(filename)\n",
    "    audio_data = audio_data.T\n",
    "    audio_data = audio_data / np.max(np.abs(audio_data))  # normalize the audio data to the range [-1, 1]\n",
    "\n",
    "    mfccs = librosa.feature.mfcc(y=audio_data, sr=rate, n_mfcc=13)\n",
    "\n",
    "    model = torch.load(\"acoustic_model.pt\")  # load a pre-trained acoustic model\n",
    "    inputs = torch.tensor(mfccs.T).unsqueeze(0)  # add a batch dimension to the inputs\n",
    "    outputs = model(inputs)  # run the inputs through the acoustic model\n",
    "    _, predicted_phonemes = torch.max(outputs, dim=2)  # get the predicted phonemes\n",
    "\n",
    "    for i, predicted_phoneme in enumerate(predicted_phonemes[0]):\n",
    "        if predicted_phoneme == \"M\":  # check if the predicted phoneme is \"M\"\n",
    "            time = i * (len(audio_data) / mfccs.shape[1]) / rate\n",
    "            print(\"Phonetic sound of M found at time: {:.2f} seconds\".format(time))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "result2:\n",
      "{   'alternative': [   {   'confidence': 0.85986006,\n",
      "                           'transcript': 'entering an error which are Enemies '\n",
      "                                         'committed look like anyone is saying '\n",
      "                                         'anything have me say things like '\n",
      "                                         'now'},\n",
      "                       {   'transcript': 'which are Enemies committed look '\n",
      "                                         'like anyone is saying anything have '\n",
      "                                         'me say things like now'},\n",
      "                       {   'transcript': 'entering an error which are Enemies '\n",
      "                                         'committed look like anyone saying '\n",
      "                                         'anything have me say things like '\n",
      "                                         'now'},\n",
      "                       {   'transcript': 'entering an error which are Enemies '\n",
      "                                         'committed look like anyone is saying '\n",
      "                                         'anything have me say things like now '\n",
      "                                         'information'},\n",
      "                       {   'transcript': 'which are Enemies committed look '\n",
      "                                         'like anyone is saying anything have '\n",
      "                                         'me say things like now information'}],\n",
      "    'final': True}\n",
      "m:\n",
      "\t0.43 - 0.57\n",
      "\t0.22 - 0.34\n",
      "\t0.34 - 0.45\n",
      "\t0.01 - 0.51\n",
      "b:\n",
      "p:\n"
     ]
    }
   ],
   "source": [
    "import speech_recognition as sr\n",
    "\n",
    "# Load the audio file into memory\n",
    "r = sr.Recognizer()\n",
    "with sr.AudioFile('transcript.wav') as source:\n",
    "    audio = r.record(source)\n",
    "\n",
    "# Recognize the speech in the audio file\n",
    "text = r.recognize_google(audio)\n",
    "\n",
    "# Split the text into words\n",
    "words = text.split()\n",
    "\n",
    "# Keep track of the time stamp for each word\n",
    "time_stamps = []\n",
    "start_time = 0\n",
    "for word in words:\n",
    "    end_time = start_time + len(word) / 11025 # Assuming a sample rate of 11025 Hz\n",
    "    time_stamps.append((start_time, end_time))\n",
    "    start_time = end_time\n",
    "\n",
    "# Look for occurrences of \"m\", \"b\", and \"p\"\n",
    "letter_time_stamps = {}\n",
    "for letter in \"mbp\":\n",
    "    letter_time_stamps[letter] = []\n",
    "\n",
    "for i, word in enumerate(words):\n",
    "    for j, char in enumerate(word):\n",
    "        if char.lower() in \"mbp\":\n",
    "            letter_time_stamps[char.lower()].append(\n",
    "                (time_stamps[i][0] + j / len(word), time_stamps[i][0] + (j + 1) / len(word))\n",
    "            )\n",
    "\n",
    "# Print the time stamps for each letter\n",
    "for letter, stamps in letter_time_stamps.items():\n",
    "    print(f\"{letter}:\")\n",
    "    for start, end in stamps:\n",
    "        print(f\"\\t{start:.2f} - {end:.2f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3rc1"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b081a66ee97bd2b6a16f43955f1d810b7ea816d6eaeb65e157ef9e038445f0c6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
