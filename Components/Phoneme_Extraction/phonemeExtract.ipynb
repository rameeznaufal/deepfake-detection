{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Python code to convert video to audio\n",
    "import moviepy.editor as mp\n",
    "import torch\n",
    "import librosa\n",
    "import numpy as np\n",
    "import soundfile as sf\n",
    "import wave\n",
    "from scipy.io import wavfile\n",
    "from IPython.display import Audio\n",
    "from transformers import Wav2Vec2ForCTC, Wav2Vec2Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Writing audio in ./transcript.wav\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                      "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    }
   ],
   "source": [
    "# Insert Local Video File Path \n",
    "clip = mp.VideoFileClip(r\"./obamaDeepfake.mp4\")\n",
    "  \n",
    "# Insert Local Audio File Path\n",
    "clip.audio.write_audiofile(r\"./transcript.wav\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'Wav2Vec2CTCTokenizer'. \n",
      "The class this function is called from is 'Wav2Vec2Tokenizer'.\n",
      "Some weights of Wav2Vec2ForCTC were not initialized from the model checkpoint at facebook/wav2vec2-base-960h and are newly initialized: ['wav2vec2.masked_spec_embed']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = Wav2Vec2Tokenizer.from_pretrained(\"facebook/wav2vec2-base-960h\")\n",
    "model = Wav2Vec2ForCTC.from_pretrained(\"facebook/wav2vec2-base-960h\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WE'RE ENTERING AN ERA IN WHICH OUR ENEMIES CAN MAKE IT LOOK LIKE ANY ONE IS SAYING ANYTHING AT ANY POINT IN TIME EVEN IF THEY WOULD NEVER SAY THOSE THINGS SO FOR INSTANCE THEY COULD HAVE ME SAY THINGS LIKE I KNOW KILMONGER WAS RIGHT OR BEN CARSON IS IN THE SUNKIM PLACE OR ABOUT THIS SIMPLY PRESIDENT TRUMP IS A TOTAL AND COMPLETE DIPSHIT NOW YOU SEE I WOULD NEVER SAYESE THINGS AT LEAST NOT IN A PUBLIC ADDRESS BUT SOME ONE ELSE WOULD SOME ONE LIKE GORDAN PEAL THIS IS A DANGEROUS TIME MOVING FORWARD WE NEED TO BE MORE VIGILANT WITH WHAT WE TRUST FROM THE INTERNET THAT'S A TIME WHEN WE NEED TO RELY ON TRUSTED NEW SOURCES MAY SOUND BASIC BUT HOW WE MOVE FORWARD IN THE AGE OF INFORMATION ISGOING TO BE THE DIFFERENCE BETWEEN WHETHER WE SURVIVE OR WHETHER WE BECOME SOME KIND OF FUCKED UP DYSTOPIA THANK YOU STAY WOKE BITCHES\n"
     ]
    }
   ],
   "source": [
    "file_name = 'transcript.wav'\n",
    "framerate = 16000\n",
    "input_audio, _ = librosa.load(file_name, sr=framerate)\n",
    "\n",
    "# Use a loop to process the audio file in segments, if it's too large to fit in memory\n",
    "segment_length = 30  # in seconds\n",
    "num_segments = int(np.ceil(len(input_audio) / (segment_length * framerate)))\n",
    "transcription = ''\n",
    "start_time = 0\n",
    "for i in range(num_segments):\n",
    "    start = i * segment_length * framerate\n",
    "    end = (i + 1) * segment_length * framerate\n",
    "    input_segment = input_audio[start:end]\n",
    "    input_values = tokenizer(input_segment, return_tensors=\"pt\").input_values\n",
    "    logits = model(input_values).logits\n",
    "    predicted_ids = torch.argmax(logits, dim=-1)\n",
    "    segment_transcription = tokenizer.batch_decode(predicted_ids)[0]\n",
    "    transcription += segment_transcription\n",
    "\n",
    "print(transcription)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "72.47\n",
      "Phoneme 'b', 'm', or 'p' found in word 'ENEMIES' at 0:3.170\n",
      "Phoneme 'b', 'm', or 'p' found in word 'MAKE' at 0:4.76\n",
      "Phoneme 'b', 'm', or 'p' found in word 'POINT' at 0:9.58\n",
      "Phoneme 'b', 'm', or 'p' found in word 'TIME' at 0:9.964\n",
      "Phoneme 'b', 'm', or 'p' found in word 'ME' at 0:16.758\n",
      "Phoneme 'b', 'm', or 'p' found in word 'KILMONGER' at 0:19.476\n",
      "Phoneme 'b', 'm', or 'p' found in word 'BEN' at 0:21.288\n",
      "Phoneme 'b', 'm', or 'p' found in word 'SUNKIM' at 0:23.552\n",
      "Phoneme 'b', 'm', or 'p' found in word 'PLACE' at 0:24.5\n",
      "Phoneme 'b', 'm', or 'p' found in word 'ABOUT' at 0:24.911\n",
      "Phoneme 'b', 'm', or 'p' found in word 'SIMPLY' at 0:25.817\n",
      "Phoneme 'b', 'm', or 'p' found in word 'PRESIDENT' at 0:26.270\n",
      "Phoneme 'b', 'm', or 'p' found in word 'TRUMP' at 0:26.723\n",
      "Phoneme 'b', 'm', or 'p' found in word 'COMPLETE' at 0:28.987\n",
      "Phoneme 'b', 'm', or 'p' found in word 'DIPSHIT' at 0:29.440\n",
      "Phoneme 'b', 'm', or 'p' found in word 'PUBLIC' at 0:35.782\n",
      "Phoneme 'b', 'm', or 'p' found in word 'BUT' at 0:36.687\n",
      "Phoneme 'b', 'm', or 'p' found in word 'SOME' at 0:37.140\n",
      "Phoneme 'b', 'm', or 'p' found in word 'SOME' at 0:38.952\n",
      "Phoneme 'b', 'm', or 'p' found in word 'PEAL' at 0:40.764\n",
      "Phoneme 'b', 'm', or 'p' found in word 'TIME' at 0:43.29\n",
      "Phoneme 'b', 'm', or 'p' found in word 'MOVING' at 0:43.481\n",
      "Phoneme 'b', 'm', or 'p' found in word 'BE' at 0:45.746\n",
      "Phoneme 'b', 'm', or 'p' found in word 'MORE' at 0:46.199\n",
      "Phoneme 'b', 'm', or 'p' found in word 'FROM' at 0:48.917\n",
      "Phoneme 'b', 'm', or 'p' found in word 'TIME' at 0:51.181\n",
      "Phoneme 'b', 'm', or 'p' found in word 'MAY' at 0:55.711\n",
      "Phoneme 'b', 'm', or 'p' found in word 'BASIC' at 0:56.617\n",
      "Phoneme 'b', 'm', or 'p' found in word 'BUT' at 0:57.70\n",
      "Phoneme 'b', 'm', or 'p' found in word 'MOVE' at 0:58.428\n",
      "Phoneme 'b', 'm', or 'p' found in word 'INFORMATION' at 1:1.146\n",
      "Phoneme 'b', 'm', or 'p' found in word 'BE' at 1:2.505\n",
      "Phoneme 'b', 'm', or 'p' found in word 'BETWEEN' at 1:3.864\n",
      "Phoneme 'b', 'm', or 'p' found in word 'BECOME' at 1:7.34\n",
      "Phoneme 'b', 'm', or 'p' found in word 'SOME' at 1:7.487\n",
      "Phoneme 'b', 'm', or 'p' found in word 'UP' at 1:9.299\n",
      "Phoneme 'b', 'm', or 'p' found in word 'DYSTOPIA' at 1:9.752\n",
      "Phoneme 'b', 'm', or 'p' found in word 'BITCHES' at 1:12.17\n"
     ]
    }
   ],
   "source": [
    "# Split the transcription into words\n",
    "words = transcription.split()\n",
    "\n",
    "# Calculate the duration of the audio data\n",
    "wav = wave.open(file_name, 'r')\n",
    "frames = wav.getnframes()\n",
    "rate = wav.getframerate()\n",
    "duration = frames / float(rate)\n",
    "print(duration)\n",
    "\n",
    "# Find the time stamps of the phonemes 'b', 'm', and 'p'\n",
    "for i, word in enumerate(words):\n",
    "    if 'B' in word or 'M' in word or 'P' in word:\n",
    "        word_duration = duration / len(words)\n",
    "        time_stamp = i * word_duration\n",
    "        minutes, seconds = divmod(time_stamp, 60)\n",
    "        milliseconds = int((seconds % 1) * 1000)\n",
    "        print(f\"Phoneme 'b', 'm', or 'p' found in word '{word}' at {int(minutes)}:{int(seconds)}.{milliseconds}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'Wav2Vec2CTCTokenizer'. \n",
      "The class this function is called from is 'Wav2Vec2Tokenizer'.\n",
      "c:\\Python\\Python38\\lib\\site-packages\\transformers\\models\\wav2vec2\\tokenization_wav2vec2.py:752: FutureWarning: The class `Wav2Vec2Tokenizer` is deprecated and will be removed in version 5 of Transformers. Please use `Wav2Vec2Processor` or `Wav2Vec2CTCTokenizer` instead.\n",
      "  warnings.warn(\n",
      "Some weights of Wav2Vec2ForCTC were not initialized from the model checkpoint at facebook/wav2vec2-base-960h and are newly initialized: ['wav2vec2.masked_spec_embed']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WE ARE ENTERING AN ERROR IN WHICH OUR ENEMIES CAN MAKE IT LOOK LIKE ANY ONE IS SAYING ANYTHING AT ANY POINT IN TIME EVEN IF THEY WOULD NEVER SAY THOSE THINGS NOW YOU SEE I WOULD NEVER SAY THESE THINGS AT LEAST NOT IN A PUBLIC ADDRESS BUT SOME ONE ELSE WOULD SOME ONE LIKE JEORDON PM THIS IS A DANGEROUS TYIME MOVING FORWARD WE NEED TO BE MORE VIGILANT WITH WHAT WE TRUST FROM THE INTERNATE AT A TIME WHEN WE NEED TO RELY ON TRUSTED NEW SOURCES THIS MAY SOUND BASIC BUT HOW WE MOVE FORWARD IN THE AGE OF INFORMATION IS GOING TO BECOME THE DIFFERENCE BETWEEN WHETHER WE SURVIVE OR WHETHER WE BECOME SOME KIND OF DISTOPIA THANK YOU\n",
      "Phoneme times:\n",
      "E: ['0.062', '0.312', '0.438', '0.625', '1.188', '2.375', '2.500', '2.688', '3.312', '4.125', '4.625', '7.125', '7.250', '7.375', '7.875', '8.500', '8.625', '9.312', '10.438', '10.500', '11.188', '11.312', '11.875', '12.000', '12.812', '14.375', '15.062', '15.312', '15.438', '15.625', '16.312', '16.562', '16.875', '17.062', '18.562', '19.188', '20.312', '20.500', '20.562', '21.000', '21.312', '22.688', '23.625', '23.938', '24.250', '24.875', '25.125', '25.375', '25.562', '25.625', '26.062', '26.812', '27.062', '27.562', '29.625', '29.938', '30.875', '31.125', '33.000', '33.250', '33.500', '33.875', '34.000', '34.188', '34.375', '34.562', '34.625', '34.938', '35.125', '35.375', '35.875', '36.312', '36.500', '36.750', '36.938', '37.188', '37.500']\n",
      "M: ['2.562', '3.125', '7.062', '15.000', '16.250', '17.562', '19.125', '19.312', '21.125', '23.375', '24.812', '28.062', '29.750', '31.750', '33.188', '37.125', '37.438']\n",
      "B: ['13.812', '14.625', '20.938', '28.688', '29.062', '32.938', '34.312', '36.875']\n",
      "P: ['6.375', '13.688', '17.500', '38.438']\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import librosa\n",
    "import numpy as np\n",
    "from transformers import Wav2Vec2Tokenizer, Wav2Vec2ForCTC\n",
    "\n",
    "# Load the audio file and resample it to the required sample rate\n",
    "audio_file = 'reuben.wav'\n",
    "sr = 16000\n",
    "audio, _ = librosa.load(audio_file, sr=sr)\n",
    "\n",
    "# Split the audio file into segments of 5 seconds\n",
    "segment_len = 60 * sr\n",
    "segments = [audio[i:i+segment_len] for i in range(0, len(audio), segment_len)]\n",
    "\n",
    "# Load the Wav2Vec2 tokenizer and model\n",
    "tokenizer = Wav2Vec2Tokenizer.from_pretrained(\"facebook/wav2vec2-base-960h\")\n",
    "model = Wav2Vec2ForCTC.from_pretrained(\"facebook/wav2vec2-base-960h\")\n",
    "\n",
    "# Define the phonemes of interest\n",
    "target_phonemes = ['E', 'M', 'B', 'P']\n",
    "\n",
    "# Initialize the output array for the phoneme times\n",
    "phoneme_times = {ph: [] for ph in target_phonemes}\n",
    "\n",
    "# Iterate over the audio segments and predict the phonemes\n",
    "for i, segment in enumerate(segments):\n",
    "    input_values = tokenizer(segment, return_tensors='pt').input_values\n",
    "    with torch.no_grad():\n",
    "        logits = model(input_values).logits\n",
    "    predicted_ids = torch.argmax(logits, dim=-1)\n",
    "    predicted_phonemes = tokenizer.batch_decode(predicted_ids)[0]\n",
    "    print(predicted_phonemes)\n",
    "\n",
    "    # Add the times of occurrence of the target phonemes to the output array\n",
    "    for ph in target_phonemes:\n",
    "        ph_indices = [j for j, p in enumerate(predicted_phonemes) if p == ph]\n",
    "        ph_times = [(i*segment_len + j)*1.0/sr for j in ph_indices]\n",
    "        phoneme_times[ph].extend(ph_times)\n",
    "\n",
    "# Print the output phoneme times\n",
    "print('Phoneme times:')\n",
    "for ph in target_phonemes:\n",
    "    for i in range(len(phoneme_times[ph])):\n",
    "        num = phoneme_times[ph][i]\n",
    "        phoneme_times[ph][i] = format((int(num) + ((num - int(num))*1000)), \".3f\")\n",
    "    print(f'{ph}: {phoneme_times[ph]}')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3rc1"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b081a66ee97bd2b6a16f43955f1d810b7ea816d6eaeb65e157ef9e038445f0c6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
