{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of Wav2Vec2ForCTC were not initialized from the model checkpoint at facebook/wav2vec2-large-960h-lv60-self and are newly initialized: ['wav2vec2.masked_spec_embed']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'Wav2Vec2CTCTokenizer'. \n",
      "The class this function is called from is 'Wav2Vec2Tokenizer'.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import librosa\n",
    "import numpy as np\n",
    "from transformers import Wav2Vec2ForCTC, Wav2Vec2Tokenizer\n",
    "\n",
    "# Load pre-trained model and tokenizer\n",
    "model = Wav2Vec2ForCTC.from_pretrained(\"facebook/wav2vec2-large-960h-lv60-self\")\n",
    "tokenizer = Wav2Vec2Tokenizer.from_pretrained(\"facebook/wav2vec2-large-960h-lv60-self\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WE'RE ENTERING AN ERA IN WHICH OUR ENEMIES CAN MAKE IT LOOK LIKE ANYONE IS SAYING ANYTHING AT ANY POINT IN TIME EVEN IF THEY WOULD NEVER SAY THOSE THINGS SO FOR INSTANCE THEY COULD HAVE ME SAY THINGS LIKE I DON'T KNOW KILLMONGER WAS RIGHT OR BEN CARSON IS IN THE SUNKIN PLACE OR HOW ABOUT THIS SIMPLY PRESIDENT TRUMP IS A TOTAL AND COMPLETE DIPSHIT NOW YOU SEE I WOULDT NEVER SAY THESE THINGS AT LEAST NOT IN A PUBLIC ADDRESS BUT SOMEONE ELSE WOULD SOMEONE LIKE GORDAN PEEL THIS IS A DANGEROUS TIME MOVING FORWARD WE NEED TO BE MORE VIGILANT WITH WHAT WE TRUST FROM THE INTERNET AT'S A TIME WHEN WE NEED TO RELY ON TRUSTED NEW SOURCES MAY SOUND BASIC BUT HOW WE MOVE FORWARD IN THE AGE OF INFORMATION ITIS GOING TO BE THE DIFFERENCE BETWEEN WHETHER WE SURVIVE OR WHETHER WE BECOME SOME KIND OF FUCKED UP DISTOPIA THANK YOU STAY WOLK PITCHES\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "# Load WAV file and extract features\n",
    "audio_file = \"transcript.wav\"\n",
    "signal, sample_rate = librosa.load(audio_file, sr=16000)\n",
    "inputs = tokenizer(signal, return_tensors=\"pt\", padding=True)\n",
    "\n",
    "# Pass features through the model to get predicted transcriptions\n",
    "with torch.no_grad():\n",
    "    outputs = model(inputs.input_values, attention_mask=inputs.attention_mask)\n",
    "\n",
    "predicted_ids = np.argmax(outputs.logits, axis=-1)\n",
    "transcription = tokenizer.batch_decode(predicted_ids)[0]\n",
    "print(transcription)\n",
    "\n",
    "# Identify timestamps of 'm' phoneme with confidence above a certain threshold\n",
    "m_indices = [i for i, (ph, conf) in enumerate(zip(transcription, outputs.logits.softmax(-1).max(-1).values)) if ph == 'M' and conf > 0.7]\n",
    "m_timestamps = [(idx * 0.02, (idx+1) * 0.02) for idx in m_indices]  # assuming frame shift of 0.02 seconds\n",
    "\n",
    "print(m_timestamps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import deepspeech\n",
    "import numpy as np\n",
    "import wave\n",
    "\n",
    "# Load the DeepSpeech model\n",
    "model = deepspeech.Model('deepspeech-0.9.3-models.pbmm')\n",
    "\n",
    "# Load the audio file\n",
    "audio_file = wave.open('transcript.wav', 'rb')\n",
    "audio_data = audio_file.readframes(audio_file.getnframes())\n",
    "\n",
    "# Convert the audio data to a NumPy array\n",
    "audio_data_np = np.frombuffer(audio_data, np.int16)\n",
    "\n",
    "# Transcribe the audio file\n",
    "transcript = model.stt(audio_data_np)\n",
    "print(transcript)\n",
    "\n",
    "# Extract the phonemes with closed mouth shapes\n",
    "closed_mouth_phonemes = ['M', 'B', 'P']\n",
    "phonemes = transcript.split(' ')\n",
    "closed_mouth_phonemes_detected = [p for p in phonemes if p in closed_mouth_phonemes]\n",
    "\n",
    "# Print the detected phonemes\n",
    "print('Closed mouth phonemes detected: ', closed_mouth_phonemes_detected)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'Wav2Vec2CTCTokenizer'. \n",
      "The class this function is called from is 'Wav2Vec2Tokenizer'.\n",
      "Some weights of Wav2Vec2ForCTC were not initialized from the model checkpoint at facebook/wav2vec2-base-960h and are newly initialized: ['wav2vec2.masked_spec_embed']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "702\n",
      "<class 'float'>\n",
      "<class 'float'>\n",
      "<class 'float'>\n",
      "134\n",
      "<class 'float'>\n",
      "<class 'float'>\n",
      "<class 'float'>\n",
      "Phoneme times:\n",
      "M: ['3.248', '4.017', '9.402', '15.983', '18.632', '24.701', '26.239', '27.949', '36.325', '38.034', '42.051', '42.308', '44.786', '47.863', '50.171', '54.188', '56.496', '59.231', '56.600', '56.874']\n",
      "B: ['20.342', '23.675', '34.701', '35.812', '44.530', '55.043', '55.556', '53.144', '54.132', '56.380', '59.616']\n",
      "P: ['8.462', '22.735', '24.786', '25.128', '26.325', '28.034', '28.718', '34.530', '39.744', '57.916', '58.300']\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import librosa\n",
    "import numpy as np\n",
    "from transformers import Wav2Vec2Tokenizer, Wav2Vec2ForCTC\n",
    "\n",
    "# Load the audio file and resample it to the required sample rate\n",
    "audio_file = 'transcript.wav'\n",
    "sr = 16000\n",
    "audio, _ = librosa.load(audio_file, sr=sr)\n",
    "\n",
    "# Split the audio file into segments of 5 seconds\n",
    "segment_len = 60 * sr\n",
    "segments = [audio[i:i+segment_len] for i in range(0, len(audio), segment_len)]\n",
    "\n",
    "# Load the Wav2Vec2 tokenizer and model\n",
    "tokenizer = Wav2Vec2Tokenizer.from_pretrained(\"facebook/wav2vec2-base-960h\")\n",
    "model = Wav2Vec2ForCTC.from_pretrained(\"facebook/wav2vec2-base-960h\")\n",
    "\n",
    "# Define the phonemes of interest\n",
    "target_phonemes = ['M', 'B', 'P']\n",
    "\n",
    "# Initialize the output array for the phoneme times\n",
    "phoneme_times = {ph: [] for ph in target_phonemes}\n",
    "\n",
    "# Iterate over the audio segments and predict the phonemes\n",
    "for i, segment in enumerate(segments):\n",
    "    input_values = tokenizer(segment, return_tensors='pt').input_values\n",
    "    with torch.no_grad():\n",
    "        logits = model(input_values).logits\n",
    "    predicted_ids = torch.argmax(logits, dim=-1)\n",
    "    predicted_phonemes = tokenizer.batch_decode(predicted_ids)[0]\n",
    "    print(len(predicted_phonemes))\n",
    "\n",
    "    # Add the times of occurrence of the target phonemes to the output array\n",
    "    for ph in target_phonemes:\n",
    "        ph_indices = [j for j, p in enumerate(predicted_phonemes) if p == ph]\n",
    "        ph_times = [((i*segment_len + j) * (1.0/sr)) for j in ph_indices]\n",
    "        end_val = ((i*segment_len + len(predicted_phonemes)) * (1.0/sr))\n",
    "        end_val = (int(end_val) + ((end_val - int(end_val))*1000))\n",
    "        print(type(end_val))\n",
    "\n",
    "        for k in range(len(ph_times)):\n",
    "            num = ph_times[k]\n",
    "            ph_times[k] = format(((int(num) + ((num - int(num))*1000)) * (segment_len/sr) * (1.0/end_val)), \".3f\")\n",
    "        phoneme_times[ph].extend(ph_times)\n",
    "\n",
    "# Print the output phoneme times\n",
    "print('Phoneme times:')\n",
    "for ph in target_phonemes:\n",
    "    print(f'{ph}: {phoneme_times[ph]}')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b081a66ee97bd2b6a16f43955f1d810b7ea816d6eaeb65e157ef9e038445f0c6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
